% Part I

\section{\texorpdfstring{}{\thesection}}

{\large \textbf{
Recall that in the midterm project you discretized one-dimensional differential equation
of u(x) with boundary values as follows
}

\begin{center}
    $\left\{\begin{tabular}{l}
        $-u''(x)=x-\frac{1}{2}, \ x \in [0,1]$ \\
        $u(0)=u(1)=0$ \\
    \end{tabular}\right.$
\end{center}

{\large \textbf{
by centered difference scheme with n interior mesh points. Use the matrix A and right hand
side vector b you obtained and write the code for power iteration or inverse iteration to
identify the largest and smallest eigenvalues of A. How do you speed up the computation?
What is the condition number of A?
}

\el

{\LARGE \textbf{Results:}}

The method of generation of A and b is exactly as explained in the Midterm project.

We will use the central difference method:
\[u^{''}_i=\frac{u_{i+1}-2u_i+u_{i+1}}{h^2}\]

From This Method, $u''$ matrix is reached by the \textbf{\emph{laplacian matrix}} as follows:

$\Delta=-
\left[
\begin{array}{c c c c c c c}
1      & -1       & 0        & 0        & 0         & \cdots & 0      \\
-1     & 2        & -1       & 0        & 0         & \cdots & 0      \\
0      & -1       & 2        & -1       & 0         & \cdots & 0      \\
\vdots  &  \ddots  & \ddots   &  \ddots  & \ddots    & \ddots & \vdots \\
\vdots  &  \vdots  & \ddots   &  \ddots  & \ddots    & \ddots & 0 \\
0  & \cdots   & \cdots   & 0   & -1        & 2      & -1     \\
0  & \cdots   & \cdots   & \cdots   & 0         & -1     & 1      \\
\end{array}
\right]
$

$-\frac{1}{h^2}\times \Delta\times u=-\frac{d^2u}{dx^2}=-u^{''}=x-\frac{1}{2} ~\Rightarrow A u=(x-\frac{1}{2})$

$A=-\frac{1}{h^2}\times \Delta , b= \overrightarrow{(x-\frac{1}{2})}$. In which $h=\frac{1}{n+1}$

I implemented the \di{Power Iteration} and \di{Inverse Iteration} in \di{Matlab} and ran it for different $n$ values.
\begin{itemize}
    \item The initial eigenvector is generated by \di{rand()} function of \di{Matlab}.
    \item The tolerance criterion is : $|\frac{\lambda_{new}-\lambda{old}}{\lambda_{new}}|<tol$ in which $tol=10^{-7}$
\end{itemize}

We can observe the results of the \di{Power Iteration} method in Figure \ref{fig:fig1_1}.

\begin{figure}[H]
    \centering
    \includegraphics[ width=4in]{figure1_1}
    \caption{Largest eigenvalue Versus n for n= 100, 200, ..., 6400}
    \label{fig:fig1_1}
\end{figure}

As we can see in the figure above,  The largest eigenvalue is increasing by matrix size and depends on the problem size.

By the \di{Inverse Power Iteration} method, the smallest eigenvalues are depicted in Figure \ref{fig:fig1_2}.

\begin{figure}[H]
    \centering
    \includegraphics[width=4in]{figure1_2}
    \caption{Smallest eigenvalue Versus n for n= 100, 200, ..., 6400}
    \label{fig:fig1_2}
\end{figure}

The smallest eigenvalue by the inverse iteration method is almost constant and does not depend on the problem size.

The condition number of the matrix is plotted in Figure \ref{fig:fig1_3}.

\begin{figure}[H]
    \centering
    \includegraphics[width=3.5in]{figure1_3}
    \caption{Condition number of matrix ``A'' Versus ``n'' for n= 100, 200, ..., 6400}
    \label{fig:fig1_3}
\end{figure}

The case for $n=6400$ has a very high condition number and might compromise the accuracy.

{\LARGE \textbf{Performance:}}

The computation time is displayed in table \ref{tab:1_2} and plotted in Figure \ref{fig:fig1_4}.

\begin{table}[H]
    \centering
    \caption{Computation Time(s)}
    \begin{tabular}{c|c|c}
    n & Power & InvPower \\
    \hline
    \hline
    100  &     0.0666  &  0.0084   \\
    200  &     0.0645  &  0.0076   \\
    400  &     0.6645  &  0.0243   \\
    800  &     2.1539  &  0.0989   \\
    1600 &     9.5770  &  0.4994   \\
    3200 &    37.9323  &  2.5916   \\
    6400 &   137.9697  & 16.0580   \\
    \end{tabular}
    \label{tab:1_2}
\end{table}



\begin{figure}[H]
    \centering
    $\begin{array}{cc}
        \includegraphics[width=3.4in]{figure1_4} &
        \includegraphics[width=3.4in]{figure1_5}
    \end{array}$
    \caption{(a) Runtime of the \di{Power iteration method}  Versus n (b) Runtime of the \di{Inverse Power iteration method}  Versus $n$}
    \label{fig:fig1_4}
\end{figure}

{\Large \textbf{Complexity:}}

Power Iteration Equation: (runtime vs n)

$  -0.000000000137460 n^3 +  0.000004356991842  n^2 -0.000712904568360 n + 0.1041636649467$

Which confirms the cubic complexity of ``Power Iteration'' \db{O($n^3$)}

Inverse Iteration Equation: (runtime vs n)

$  0.000000000044847 n^3 +  0.000000100028964  n^2 + 0.000032380241030 n -0.003075456342914$

Which confirms the cubic complexity of the ``Inverse Iteration'' \db{O($n^3$)}

The number of tries for each step of the both methods is on Table \ref{tab:1_1}.


\begin{table}[H]
    \centering
    \caption{Number of iterations needed}
    \begin{tabular}{c|c|c}
    n & Power & InvPower\\
    \hline
    \hline
    100  & 2016 & 4  \\
    200  & 1101 & 4  \\
    400  & 2159 & 4  \\
    800  & 1790 & 4  \\
    1600 & 1462 & 4  \\
    3200 & 1441 & 4  \\
    6400 & 1393 & 4  \\
    \end{tabular}
    \label{tab:1_1}
\end{table}


Convergence of the error of \di{Power iteration} and \di{Inverse iteration} are depicted in Figure \ref{fig:fig1_6}. The y axis is plotted log-scale.


\begin{figure}[H]
    \centering
    $\begin{array}{cc}
        \includegraphics[width=3.4in]{figure1_6} &
        \includegraphics[width=3.4in]{figure1_7}
    \end{array}$
    \caption{Convergence of the percentage of error by iteration step for $n=800$, (a)Power Iteration Method, (b)Inverse Iteration Method.}
    \label{fig:fig1_6}
\end{figure}

 {\LARGE \textbf{Observations \slash~  Explanations:}}

\begin{itemize}
    \item As we can see, the \di{Inverse Iteration} method is converging \db{linear in log-scale}, that is, it is converging very stronger than the Power iteration convergence. That can be because the smallest e.w. and the second smallest are very different:
        
        $\lambda_{m-1}\slash\lambda_{m} >>1$
        
        while
        
        $\lambda_2 \slash \lambda_1 $ is close to 1 \\
        In which $\lambda_1 $ is the largest eigenvalue
        and the convergence rate is $O((\lambda_{k} \slash \lambda_{k-1})^{2k}) $
        
        \db{That is why we see an exponential convergence from Inverse Iteration and slower convergence from Power Iteration.}
        
    \item Although each step of \di{Inverse Iteration} is heavier than one step of \di{Power iteration}, it takes only a few steps, versus thousands of \di{Power Iteration} steps, and that makes it in general very faster, as obvious in Figure \ref{fig:fig1_4}.
    \item The minimum eigenvalue depends on the nature of the problem, not the size. So we see it remains constant. On the other hand, The Largest e.w. increases with size and depends on $n$.
    
\end{itemize}


 {\LARGE \textbf{How to Speed up:}}

\begin{itemize}
    \item Currently, I am using $\mu=\lambda$. We can set $\mu=\lambda$ at each step. With the use of that, the performance will be very faster(that will be the reileigh quotient iteration with $O((\lambda^{(k)}- \lambda_J)^{3})$ convergence rate. which will asymptotically be $O(\epsilon^3)$, because the difference between $\lambda^{(k)}$ and $\lambda_J$ approaches 0.
    \item Pre-conditioning is also applicable to both methods for faster convergence.
\end{itemize}

% END------------------------------------------------------------
\begin{comment}
\vspace{\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=7in]{figure1_2}
    %\epsfig{file=figure.eps,width=0.9\linewidth,clip=}
    \caption{u versus x for n=50, 100, 200, ..., 6400}
    \label{fig:fig1_2}
\end{figure}
%}
\begin{table}[H]
    \centering
    \caption{Loss of orthogonality index}
    \begin{tabular}{c|c|c}
    ~              & $n=100$      &  $n=150$ \\
    \hline
    \hline
    Classical GS   & 74.40        &  123.25        \\
    Modified GS    & 0.9990        &  0.9999        \\
    Householder    & 3.50 e-15     &   3.87 e-15       \\
    \end{tabular}
    \label{tab:2_2}
\end{table}
\end{comment}
